{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Style_Transfer.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lbenbaccar/Neural-Style-Transfer-using-CNN/blob/main/Neural_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3KFVkLPRNPm"
      },
      "source": [
        "# A Neural Algorithm of Artistic Style PyTorch Implementation\n",
        "## **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOxhqAZ42jiZ"
      },
      "source": [
        "import os\n",
        "from os import path\n",
        "import copy\n",
        "from sys import version_info\n",
        "from collections import OrderedDict\n",
        "import argparse\n",
        "from PIL import Image\n",
        "Image.MAX_IMAGE_PIXELS = 1000000000 \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.model_zoo import load_url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvBTmAq-qQO-"
      },
      "source": [
        "# **Model downloads**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU1kS-H2qWKY"
      },
      "source": [
        "## Download the VGG-19 model and fix the layer names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiVms7QVqPc4"
      },
      "source": [
        "sd = load_url(\"https://web.eecs.umich.edu/~justincj/models/vgg19-d01eb7cb.pth\")\n",
        "map = {'classifier.1.weight':u'classifier.0.weight', 'classifier.1.bias':u'classifier.0.bias', 'classifier.4.weight':u'classifier.3.weight', 'classifier.4.bias':u'classifier.3.bias'}\n",
        "sd = OrderedDict([(map[k] if k in map else k,v) for k,v in sd.items()])\n",
        "torch.save(sd, \"vgg19-d01eb7cb.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll2JoVHmqa6V"
      },
      "source": [
        "## Download the VGG-16 model and fix the ayer names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oIGiXTDqaLt"
      },
      "source": [
        "sd = load_url(\"https://web.eecs.umich.edu/~justincj/models/vgg16-00b39a1b.pth\")\n",
        "map = {'classifier.1.weight':u'classifier.0.weight', 'classifier.1.bias':u'classifier.0.bias', 'classifier.4.weight':u'classifier.3.weight', 'classifier.4.bias':u'classifier.3.bias'}\n",
        "sd = OrderedDict([(map[k] if k in map else k,v) for k,v in sd.items()])\n",
        "torch.save(sd, \"vgg16-00b39a1b.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ9SCGY_qf94"
      },
      "source": [
        "## Download the NIN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT8S_z2wqh_-"
      },
      "source": [
        "if version_info[0] < 3:\n",
        "  import urllib\n",
        "  urllib.URLopener().retrieve(\"https://raw.githubusercontent.com/ProGamerGov/pytorch-nin/master/nin_imagenet.pth\", \"nin_imagenet.pth\")\n",
        "else: \n",
        "  import urllib.request\n",
        "  urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ProGamerGov/pytorch-nin/master/nin_imagenet.pth\", \"nin_imagenet.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPVxqr08RAhQ"
      },
      "source": [
        "# **Options**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7De8XuPRt4R"
      },
      "source": [
        "parser = argparse.ArgumentParser()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px_cp9fNRreG"
      },
      "source": [
        "## Basic options\n",
        "\n",
        "*   `-style_image`: path of the style image.\n",
        "*   `-content_image`: path of the content image.\n",
        "*   `-style_blend_weights`: weight for blending the style of multiple style images. *Format :* comma-separated list. *Example :* `-style_blend_weights` 2,6,7. (*by default :* all style images equally weighted)\n",
        "*   `-image_size`: maximum side length in pixels of the generated image. (*by default :* 512)\n",
        "*   `-gpu`: zero-indexed ID of the GPU to use.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sRdkTTaRLgj",
        "outputId": "684275c1-0713-4703-f34f-9ab050feddb7"
      },
      "source": [
        "parser.add_argument(\"-style_image\", help=\"Style target image\", default='style6.jpg')\n",
        "parser.add_argument(\"-content_image\", help=\"Content target image\", default='ensae.jpg')\n",
        "parser.add_argument(\"-style_blend_weights\", default=None)\n",
        "parser.add_argument(\"-image_size\", help=\"Maximum height / width of generated image\", type=int, default=512)\n",
        "parser.add_argument(\"-gpu\", help=\"Zero-indexed ID of the GPU to use; for CPU mode set -gpu = c\", default=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-gpu'], dest='gpu', nargs=None, const=None, default=0, type=None, choices=None, help='Zero-indexed ID of the GPU to use; for CPU mode set -gpu = c', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wqd27GiVYfC"
      },
      "source": [
        "## Optimization options\n",
        "For the optimization algorithm, L-BFGS tends to give better results, but uses more memory. ADAM reduce memory usage. Moreover, when using ADAM we need to play with other parameters to get good results, especially the style weight, content weight, and learning rate.\n",
        "\n",
        "*   `-content_weight`: argument to know how much to weight the content reconstruction term. (*by default:* 5e0)\n",
        "*   `-style_weight`: argument to know how much to weight the style reconstruction term. (*by default:* 1e2)\n",
        "*   `-tv_weight`: weight of total-variation (TV) regularization to help smoothing the image. 0 to disable TV regularization. (*by default:* 1e-3)\n",
        "*   `-num_iterations`: number of iterations. (*by default:* 1000)\n",
        "*   `-init`: method for generating the generated image, one of `random` or `image`. (*by default:* `random` which uses a noise initialization; `image` initializes with the content image)\n",
        "*   `-init_image`: replaces the initialization image with a user specified image.\n",
        "*   `-optimizer`: optimization algorithm to use, `lbfgs` or `adam`. (*by default:* `lbfgs`)\n",
        "*   `-learning_rate`: learning rate to use with the ADAM optimizer (*by default:* 1e1)\n",
        "*   `-lbfgs_num_correction`: number of correction pairs stored. (*by default:* 100)\n",
        "*   `-normalize_weights`: if present, weights will be L1 normalized.\n",
        "*   `-normalize_gradients`: if present, style and content gradients from each layer will be L1 normalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6Pgxse1Ym_w",
        "outputId": "886ae4f6-7215-4e04-ae0b-834cfd67bc8e"
      },
      "source": [
        "parser.add_argument(\"-content_weight\", type=float, default=5e0)\n",
        "parser.add_argument(\"-style_weight\", type=float, default=1e2)\n",
        "parser.add_argument(\"-tv_weight\", type=float, default=1e-3)\n",
        "parser.add_argument(\"-num_iterations\", type=int, default=1000)\n",
        "parser.add_argument(\"-init\", choices=['random', 'image'], default='random')\n",
        "parser.add_argument(\"-init_image\", default=None)\n",
        "parser.add_argument(\"-optimizer\", choices=['lbfgs', 'adam'], default='lbfgs')\n",
        "parser.add_argument(\"-learning_rate\", type=float, default=1e1)\n",
        "parser.add_argument(\"-lbfgs_num_correction\", type=int, default=100)\n",
        "parser.add_argument(\"-normalize_weights\", action='store_true')\n",
        "parser.add_argument(\"-normalize_gradients\", action='store_true')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreTrueAction(option_strings=['-normalize_gradients'], dest='normalize_gradients', nargs=0, const=True, default=False, type=None, choices=None, help=None, metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1YIsiKlYM-D"
      },
      "source": [
        "## Output options\n",
        "\n",
        "*   `-output_image`: name of the output image (*by default:* `out.png`)\n",
        "*   `-print_iter`: print progress every `print_iter` iterations. 0 to disable printing.\n",
        "*   `-save_iter`: save the image every `save_iter` iterations. Set to 0 to disable saving intermediate results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZFg8RrNaIWU",
        "outputId": "1eb46e83-71fc-4774-fedc-a461b365ee90"
      },
      "source": [
        "parser.add_argument(\"-output_image\", default='out5.png')\n",
        "parser.add_argument(\"-print_iter\", type=int, default=100)\n",
        "parser.add_argument(\"-save_iter\", type=int, default=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-save_iter'], dest='save_iter', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, help=None, metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeQvzhSydL8h"
      },
      "source": [
        "## Layer options\n",
        " \n",
        "*   `-content_layers`: comma-separated list of layer names to use for content reconstruction (*by default:* `relu4_2`)\n",
        "*   `-style_layers`: comma-separated list of layer names to use for style reconstruction (*by default:* `relu1_1`, `relu2_1`,`relu3_1`,`relu4_1`,`relu5_1`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtRZRa37dMTT",
        "outputId": "f29d574d-87de-4dd3-8e3e-b4803baba9dd"
      },
      "source": [
        "parser.add_argument(\"-content_layers\", help=\"layers for content\", default='relu4_2')\n",
        "parser.add_argument(\"-style_layers\", help=\"layers for style\", default='relu1_1,relu2_1,relu3_1,relu4_1,relu5_1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-style_layers'], dest='style_layers', nargs=None, const=None, default='relu1_1,relu2_1,relu3_1,relu4_1,relu5_1', type=None, choices=None, help='layers for style', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPjnPTAcYk1d"
      },
      "source": [
        "## Other options\n",
        " \n",
        "*   `-style_scale`: scale at which to extract features from the style image. (*by default:* 1.0)\n",
        "*   `-original_colors`: if set to 1, then the output image will keep the colors of the content image.\n",
        "*   `-model_file`: path to the `.pth` file for the VGG Caffe model. (*by default:* the original VGG-19 model)\n",
        "*   `-pooling`: the type of pooling layers to use, one of `max` or `avg`. (*by default:* `max`). Indeed, VGG-19 models uses max pooling layers, but the paper mentions that replacing these layers with average pooling layers can improve the results.\n",
        "*   `-seed`: an integer value that we can specify for repeatable results. (*by default:* random for each run)\n",
        "*   `-multidevice_strategy`: a comma-separated list of layer indices at which to split the network when using multiple devices.\n",
        "*   `-backend`: `nn`, `cudnn`, `openmp`, or `mkl`. (*by default:* `nn`)\n",
        "*   `-cudnn_autotune`: when using the cuDNN backend, we pass this flag to use the built-in cuDNN autotuner to select the best convolution algorithms for the architecture. This will make the first iteration a bit slower and can take a bit more memory, but may significantly speed up the cuDNN backend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjNVrAgSc0a9",
        "outputId": "dab4ebae-60ca-4fc2-f407-52a9b9872589"
      },
      "source": [
        "parser.add_argument(\"-style_scale\", type=float, default=1.0)\n",
        "parser.add_argument(\"-original_colors\", type=int, choices=[0, 1], default=0)\n",
        "parser.add_argument(\"-model_file\", type=str, default='vgg19-d01eb7cb.pth')\n",
        "parser.add_argument(\"-pooling\", choices=['avg', 'max'], default='max')\n",
        "parser.add_argument(\"-seed\", type=int, default=-1)\n",
        "parser.add_argument(\"-multidevice_strategy\", default='4,7,29')\n",
        "parser.add_argument(\"-backend\", choices=['nn', 'cudnn', 'mkl', 'mkldnn', 'openmp', 'mkl,cudnn', 'cudnn,mkl'], default='cudnn')\n",
        "parser.add_argument(\"-cudnn_autotune\", action='store_true')\n",
        "parser.add_argument(\"-disable_check\", action='store_true')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreTrueAction(option_strings=['-disable_check'], dest='disable_check', nargs=0, const=True, default=False, type=None, choices=None, help=None, metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbId7_B8goV5"
      },
      "source": [
        "params, unknown = parser.parse_known_args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCjWnRqtga8N"
      },
      "source": [
        "# **Architecture classes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJlZAdc_BUR-"
      },
      "source": [
        "## Architectures parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_9ddxT1-VDG"
      },
      "source": [
        "channel_list = {\n",
        "'VGG-16p': [24, 22, 'P', 41, 51, 'P', 108, 89, 111, 'P', 184, 276, 228, 'P', 512, 512, 512, 'P'],\n",
        "'VGG-16': [64, 64, 'P', 128, 128, 'P', 256, 256, 256, 'P', 512, 512, 512, 'P', 512, 512, 512, 'P'],\n",
        "'VGG-19': [64, 64, 'P', 128, 128, 'P', 256, 256, 256, 256, 'P', 512, 512, 512, 512, 'P', 512, 512, 512, 512, 'P'],\n",
        "}\n",
        "\n",
        "vgg16_dict = {\n",
        "'C': ['conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1', 'conv3_2', 'conv3_3', 'conv4_1', 'conv4_2', 'conv4_3', 'conv5_1', 'conv5_2', 'conv5_3'],\n",
        "'R': ['relu1_1', 'relu1_2', 'relu2_1', 'relu2_2', 'relu3_1', 'relu3_2', 'relu3_3', 'relu4_1', 'relu4_2', 'relu4_3', 'relu5_1', 'relu5_2', 'relu5_3'],\n",
        "'P': ['pool1', 'pool2', 'pool3', 'pool4', 'pool5'],\n",
        "}\n",
        "\n",
        "vgg19_dict = {\n",
        "'C': ['conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1', 'conv3_2', 'conv3_3', 'conv3_4', 'conv4_1', 'conv4_2', 'conv4_3', 'conv4_4', 'conv5_1', 'conv5_2', 'conv5_3', 'conv5_4'],\n",
        "'R': ['relu1_1', 'relu1_2', 'relu2_1', 'relu2_2', 'relu3_1', 'relu3_2', 'relu3_3', 'relu3_4', 'relu4_1', 'relu4_2', 'relu4_3', 'relu4_4', 'relu5_1', 'relu5_2', 'relu5_3', 'relu5_4'],\n",
        "'P': ['pool1', 'pool2', 'pool3', 'pool4', 'pool5'],\n",
        "}\n",
        "\n",
        "nin_dict = {\n",
        "'C': ['conv1', 'cccp1', 'cccp2', 'conv2', 'cccp3', 'cccp4', 'conv3', 'cccp5', 'cccp6', 'conv4-1024', 'cccp7-1024', 'cccp8-1024'],\n",
        "'R': ['relu0', 'relu1', 'relu2', 'relu3', 'relu5', 'relu6', 'relu7', 'relu8', 'relu9', 'relu10', 'relu11', 'relu12'],\n",
        "'P': ['pool1', 'pool2', 'pool3', 'pool4'],\n",
        "'D': ['drop'],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71wETEQ0BAZb"
      },
      "source": [
        "## VGG-16 and VGG-19 architecture class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llF_xTqVzj5k"
      },
      "source": [
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, features, num_classes = 1000):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REkGKMGeE4oK"
      },
      "source": [
        "## VGG-16 architecture class using the channel pruning model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om3Ddjwz4HaY"
      },
      "source": [
        "class VGG_Pruned(nn.Module):\n",
        "  \n",
        "    def __init__(self, features, num_classes=1000):\n",
        "        super(VGG_Pruned, self).__init__()\n",
        "        self.features = features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gt03D_7FRLr"
      },
      "source": [
        "## VGG-16 architecture class using the fcn32s-heavy-pascal model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMK4Q3SH3wW1"
      },
      "source": [
        "class VGG_Fully_Convolutional_Network_32S(nn.Module):\n",
        "  \n",
        "    def __init__(self, features, num_classes=1000):\n",
        "        super(VGG_Fully_Convolutional_Network_32S, self).__init__()\n",
        "        self.features = features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv2d(512,4096,(7, 7)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Conv2d(4096,4096,(1, 1)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa8JLwl1FSg3"
      },
      "source": [
        "## VGG-16 architecture class using the SOD fintune model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FaJFoeB3XKY"
      },
      "source": [
        "class VGG_Salient_Object_Detection(nn.Module):\n",
        "  \n",
        "    def __init__(self, features, num_classes=100):\n",
        "        super(VGG_Salient_Object_Detection, self).__init__()\n",
        "        self.features = features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 100),\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItE4TDyPGR1E"
      },
      "source": [
        "## NIN architecture class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anc8Ux6A4mkc"
      },
      "source": [
        "class Network_in_Network(nn.Module):\n",
        "\n",
        "    def __init__(self, pooling):\n",
        "        super(Network_in_Network, self).__init__()\n",
        "        if pooling == 'max':\n",
        "            pool2d = nn.MaxPool2d((3, 3),(2, 2),(0, 0),ceil_mode=True)\n",
        "        elif pooling == 'avg':\n",
        "            pool2d = nn.AvgPool2d((3, 3),(2, 2),(0, 0),ceil_mode=True)\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3,96,(11, 11),(4, 4)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(96,96,(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(96,96,(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            pool2d,\n",
        "            nn.Conv2d(96,256,(5, 5),(1, 1),(2, 2)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256,256,(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256,256,(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            pool2d,\n",
        "            nn.Conv2d(256,384,(3, 3),(1, 1),(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384,384,(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384,384,(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            pool2d,\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Conv2d(384,1024,(3, 3),(1, 1),(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(1024,1024,(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(1024,1000,(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AvgPool2d((6, 6),(1, 1),(0, 0),ceil_mode=True),\n",
        "            nn.Softmax(),\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq7Eym_GG45a"
      },
      "source": [
        "class ModelParallel(nn.Module):\n",
        "    def __init__(self, net, device_ids, device_splits):\n",
        "        super(ModelParallel, self).__init__()\n",
        "        self.device_list = self.name_devices(device_ids.split(','))\n",
        "        self.chunks = self.chunks_to_devices(self.split_net(net, device_splits.split(',')))\n",
        "\n",
        "    def name_devices(self, input_list):\n",
        "        device_list = []\n",
        "        for i, device in enumerate(input_list):\n",
        "            if str(device).lower() != 'c':\n",
        "                device_list.append(\"cuda:\" + str(device))\n",
        "            else:\n",
        "                device_list.append(\"cpu\")\n",
        "        return device_list\n",
        "\n",
        "    def split_net(self, net, device_splits):\n",
        "        chunks, cur_chunk = [], nn.Sequential()\n",
        "        for i, l in enumerate(net):\n",
        "            cur_chunk.add_module(str(i), net[i])\n",
        "            if str(i) in device_splits and device_splits != '':\n",
        "                del device_splits[0]\n",
        "                chunks.append(cur_chunk)\n",
        "                cur_chunk = nn.Sequential()\n",
        "        chunks.append(cur_chunk)\n",
        "        return chunks\n",
        "\n",
        "    def chunks_to_devices(self, chunks):\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk.to(self.device_list[i])\n",
        "        return chunks\n",
        "\n",
        "    def c(self, input, i):\n",
        "        if input.type() == 'torch.FloatTensor' and 'cuda' in self.device_list[i]:\n",
        "            input = input.type('torch.cuda.FloatTensor')\n",
        "        elif input.type() == 'torch.cuda.FloatTensor' and 'cpu' in self.device_list[i]:\n",
        "            input = input.type('torch.FloatTensor')\n",
        "        return input\n",
        "\n",
        "    def forward(self, input):\n",
        "        for i, chunk in enumerate(self.chunks):\n",
        "            if i < len(self.chunks) -1:\n",
        "                input = self.c(chunk(self.c(input, i).to(self.device_list[i])), i+1).to(self.device_list[i+1])\n",
        "            else:\n",
        "                input = chunk(input)\n",
        "        return input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhs9UJZA-zMY"
      },
      "source": [
        "def buildSequential(channel_list, pooling):\n",
        "\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "\n",
        "    if pooling == 'max':\n",
        "        pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    elif pooling == 'avg':\n",
        "        pool2d = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unrecognized pooling parameter\")\n",
        "\n",
        "    for c in channel_list:\n",
        "        if c == 'P':\n",
        "            layers = layers + [pool2d]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, c, kernel_size=3, padding=1)\n",
        "            layers = layers + [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = c\n",
        "\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TlS42qoAlVa"
      },
      "source": [
        "## Function to select the model to use "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL0KlrIq-bYu"
      },
      "source": [
        "def modelSelector(model_file, pooling):\n",
        "\n",
        "    vgg_list = [\"vgg\", \"fcn32s\", \"pruning\", \"sod\"]\n",
        "\n",
        "    if any(name in model_file for name in vgg_list):\n",
        "        if \"pruning\" in model_file:\n",
        "            print(\"VGG-16 Architecture Detected\")\n",
        "            print(\"Using The Channel Pruning Model\")\n",
        "            cnn, layerList = VGG_Pruned(buildSequential(channel_list['VGG-16p'], pooling)), vgg16_dict\n",
        "\n",
        "        elif \"fcn32s\" in model_file:\n",
        "            print(\"VGG-16 Architecture Detected\")\n",
        "            print(\"Using the fcn32s-heavy-pascal Model\")\n",
        "            cnn, layerList = VGG_Fully_Convolutional_Network_32S(buildSequential(channel_list['VGG-16'], pooling)), vgg16_dict\n",
        "\n",
        "        elif \"sod\" in model_file:\n",
        "            print(\"VGG-16 Architecture Detected\")\n",
        "            print(\"Using The SOD Fintune Model\")\n",
        "            cnn, layerList = VGG_Salient_Object_Detection(buildSequential(channel_list['VGG-16'], pooling)), vgg16_dict\n",
        "\n",
        "        elif \"19\" in model_file:\n",
        "            print(\"VGG-19 Architecture Detected\")\n",
        "            cnn, layerList = VGG(buildSequential(channel_list['VGG-19'], pooling)), vgg19_dict\n",
        "\n",
        "        elif \"16\" in model_file:\n",
        "            print(\"VGG-16 Architecture Detected\")\n",
        "            cnn, layerList = VGG(buildSequential(channel_list['VGG-16'], pooling)), vgg16_dict\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"VGG architecture not recognized.\")\n",
        "\n",
        "    elif \"nin\" in model_file:\n",
        "        print(\"NIN Architecture Detected\")\n",
        "        cnn, layerList = Network_in_Network(pooling), nin_dict\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Model architecture not recognized.\")\n",
        "\n",
        "    return cnn, layerList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBXHumstANUd"
      },
      "source": [
        "## Function to print the caffe model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph6gISOK_vy2"
      },
      "source": [
        "def print_caffe(cnn, layerList):\n",
        "    c = 0\n",
        "    for l in list(cnn):\n",
        "         if \"Conv2d\" in str(l):\n",
        "             in_c, out_c, ks  = str(l.in_channels), str(l.out_channels), str(l.kernel_size)\n",
        "             print(layerList['C'][c] +\": \" +  (out_c + \" \" + in_c + \" \" + ks).replace(\")\",'').replace(\"(\",'').replace(\",\",'') )\n",
        "             c+=1\n",
        "         if c == len(layerList['C']):\n",
        "             break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYsPvE-UAB1A"
      },
      "source": [
        "## Function to load the model and configure pooling layer type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDyMVOO-_x9j"
      },
      "source": [
        "def load_caffe_model(model_file, pooling, use_gpu, disable_check):\n",
        "    cnn, layerList = modelSelector(str(model_file).lower(), pooling)\n",
        "\n",
        "    cnn.load_state_dict(torch.load(model_file), strict=(not disable_check))\n",
        "    print(\"Successfully loaded \" + str(model_file))\n",
        "\n",
        "    # Convert the model to cuda\n",
        "    if \"c\" not in str(use_gpu).lower() or \"c\" not in str(use_gpu[0]).lower():\n",
        "        cnn = cnn.cuda()\n",
        "    cnn = cnn.features\n",
        "\n",
        "    print_caffe(cnn, layerList)\n",
        "\n",
        "    return cnn, layerList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzXr7CfdhSer"
      },
      "source": [
        "# **Other classes**\n",
        "## Class to scale gradients in the backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WAeyH9BipZF"
      },
      "source": [
        "class ScaleGradients(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(self, input_tensor, strength):\n",
        "        self.strength = strength\n",
        "        return input_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(self, grad_output):\n",
        "        grad_input = grad_output.clone()\n",
        "        grad_input = grad_input / (torch.norm(grad_input, keepdim=True) + 1e-8)\n",
        "        return grad_input * self.strength * self.strength, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjGCyE7yiimc"
      },
      "source": [
        "## Class to compute content loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYIMlsGbhbBa"
      },
      "source": [
        "class ContentLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, strength, normalize):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.strength = strength\n",
        "        self.crit = nn.MSELoss()\n",
        "        self.mode = 'None'\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.mode == 'loss':\n",
        "            loss = self.crit(input, self.target)\n",
        "            if self.normalize:\n",
        "                loss = ScaleGradients.apply(loss, self.strength)\n",
        "            self.loss = loss * self.strength\n",
        "        elif self.mode == 'capture':\n",
        "            self.target = input.detach()\n",
        "        return input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9jBBWC8hjAT"
      },
      "source": [
        "## Class to compute the Gram matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxJITIzThdma"
      },
      "source": [
        "class GramMatrix(nn.Module):\n",
        "\n",
        "    def forward(self, input):\n",
        "        B, C, H, W = input.size()\n",
        "        x_flat = input.view(C, H * W)\n",
        "        return torch.mm(x_flat, x_flat.t())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XArt2Rjhysg"
      },
      "source": [
        "## Class to compute style loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVRl76xvh1WB"
      },
      "source": [
        "class StyleLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, strength, normalize):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.target = torch.Tensor()\n",
        "        self.strength = strength\n",
        "        self.gram = GramMatrix()\n",
        "        self.crit = nn.MSELoss()\n",
        "        self.mode = 'None'\n",
        "        self.blend_weight = None\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.G = self.gram(input)\n",
        "        self.G = self.G.div(input.nelement())\n",
        "        if self.mode == 'capture':\n",
        "            if self.blend_weight == None:\n",
        "                self.target = self.G.detach()\n",
        "            elif self.target.nelement() == 0:\n",
        "                self.target = self.G.detach().mul(self.blend_weight)\n",
        "            else:\n",
        "                self.target = self.target.add(self.blend_weight, self.G.detach())\n",
        "        elif self.mode == 'loss':\n",
        "            loss = self.crit(self.G, self.target)\n",
        "            if self.normalize:\n",
        "                loss = ScaleGradients.apply(loss, self.strength)\n",
        "            self.loss = self.strength * loss\n",
        "        return input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT87hMkbh3jH"
      },
      "source": [
        "## Class to compute total-variation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGdej80nh8Jv"
      },
      "source": [
        "class TVLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, strength):\n",
        "        super(TVLoss, self).__init__()\n",
        "        self.strength = strength\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.x_diff = input[:,:,1:,:] - input[:,:,:-1,:]\n",
        "        self.y_diff = input[:,:,:,1:] - input[:,:,:,:-1]\n",
        "        self.loss = self.strength * (torch.sum(torch.abs(self.x_diff)) + torch.sum(torch.abs(self.y_diff)))\n",
        "        return input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2XXOgiskOEc"
      },
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3ggBPEGkITk"
      },
      "source": [
        "def setup_gpu():\n",
        "    def setup_cuda():\n",
        "        if 'cudnn' in params.backend:\n",
        "            torch.backends.cudnn.enabled = True\n",
        "            if params.cudnn_autotune:\n",
        "                torch.backends.cudnn.benchmark = True\n",
        "        else:\n",
        "            torch.backends.cudnn.enabled = False\n",
        "\n",
        "    def setup_cpu():\n",
        "        if 'mkl' in params.backend and 'mkldnn' not in params.backend:\n",
        "            torch.backends.mkl.enabled = True\n",
        "        elif 'mkldnn' in params.backend:\n",
        "            raise ValueError(\"MKL-DNN is not supported yet.\")\n",
        "        elif 'openmp' in params.backend:\n",
        "            torch.backends.openmp.enabled = True\n",
        "\n",
        "    multidevice = False\n",
        "    if \",\" in str(params.gpu):\n",
        "        devices = params.gpu.split(',')\n",
        "        multidevice = True\n",
        "\n",
        "        if 'c' in str(devices[0]).lower():\n",
        "            backward_device = \"cpu\"\n",
        "            setup_cuda(), setup_cpu()\n",
        "        else:\n",
        "            backward_device = \"cuda:\" + devices[0]\n",
        "            setup_cuda()\n",
        "        dtype = torch.FloatTensor\n",
        "\n",
        "    elif \"c\" not in str(params.gpu).lower():\n",
        "        setup_cuda()\n",
        "        dtype, backward_device = torch.cuda.FloatTensor, \"cuda:\" + str(params.gpu)\n",
        "    else:\n",
        "        setup_cpu()\n",
        "        dtype, backward_device = torch.FloatTensor, \"cpu\"\n",
        "    return dtype, multidevice, backward_device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GAgTbPwkSa-"
      },
      "source": [
        "def setup_multi_device(net):\n",
        "    assert len(params.gpu.split(',')) - 1 == len(params.multidevice_strategy.split(',')), \\\n",
        "      \"The number of -multidevice_strategy layer indices minus 1, must be equal to the number of -gpu devices.\"\n",
        "\n",
        "    new_net = ModelParallel(net, params.gpu, params.multidevice_strategy)\n",
        "    return new_net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-mXyZaal4zV"
      },
      "source": [
        "# **Other functions**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufk_bTd-dkwa"
      },
      "source": [
        "## Preprocess an image before passing it to a model.\n",
        "We need to rescale from [0, 1] to [0, 255], convert from RGB to BGR, and subtract the mean pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iAdueFkl_xL"
      },
      "source": [
        "def preprocess(image_name, image_size):\n",
        "    image = Image.open(image_name).convert('RGB')\n",
        "    if type(image_size) is not tuple:\n",
        "        image_size = tuple([int((float(image_size) / max(image.size))*x) for x in (image.height, image.width)])\n",
        "    Loader = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor()])\n",
        "    rgb2bgr = transforms.Compose([transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])])])\n",
        "    Normalize = transforms.Compose([transforms.Normalize(mean=[103.939, 116.779, 123.68], std=[1,1,1])])\n",
        "    tensor = Normalize(rgb2bgr(Loader(image) * 255)).unsqueeze(0)\n",
        "    return tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oYDMM_GmR5p"
      },
      "source": [
        "## Cancel the previous preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTAu6Vu7mQuJ"
      },
      "source": [
        "def deprocess(output_tensor):\n",
        "    Normalize = transforms.Compose([transforms.Normalize(mean=[-103.939, -116.779, -123.68], std=[1,1,1])])\n",
        "    bgr2rgb = transforms.Compose([transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])])])\n",
        "    output_tensor = bgr2rgb(Normalize(output_tensor.squeeze(0).cpu())) / 255\n",
        "    output_tensor.clamp_(0, 1)\n",
        "    Image2PIL = transforms.ToPILImage()\n",
        "    image = Image2PIL(output_tensor.cpu())\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSiz2VjcoWRK"
      },
      "source": [
        "## Combine the Y channel of the generated image and the UV/CbCr channels of the content image to perform color-independent style transfer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQIdSxcZoaQd"
      },
      "source": [
        "def original_colors(content, generated):\n",
        "    content_channels = list(content.convert('YCbCr').split())\n",
        "    generated_channels = list(generated.convert('YCbCr').split())\n",
        "    content_channels[0] = generated_channels[0]\n",
        "    return Image.merge('YCbCr', content_channels).convert('RGB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9MBYEy_owQw"
      },
      "source": [
        "## Configure the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXVuwG2moxzV"
      },
      "source": [
        "def setup_optimizer(img):\n",
        "    if params.optimizer == 'lbfgs':\n",
        "        print(\"Running optimization with L-BFGS\")\n",
        "        optim_state = {\n",
        "            'max_iter': params.num_iterations,\n",
        "            'tolerance_change': -1,\n",
        "            'tolerance_grad': -1,\n",
        "        }\n",
        "        if params.lbfgs_num_correction != 100:\n",
        "            optim_state['history_size'] = params.lbfgs_num_correction\n",
        "        optimizer = optim.LBFGS([img], **optim_state)\n",
        "        loopVal = 1\n",
        "    elif params.optimizer == 'adam':\n",
        "        print(\"Running optimization with ADAM\")\n",
        "        optimizer = optim.Adam([img], lr = params.learning_rate)\n",
        "        loopVal = params.num_iterations - 1\n",
        "    return optimizer, loopVal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT9KH4x4obxV"
      },
      "source": [
        "## Print torch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5M4fZ4woiuG"
      },
      "source": [
        "def print_torch(net, multidevice):\n",
        "    if multidevice:\n",
        "        return\n",
        "    simplelist = \"\"\n",
        "    for i, layer in enumerate(net, 1):\n",
        "        simplelist = simplelist + \"(\" + str(i) + \") -> \"\n",
        "    print(\"nn.Sequential ( \\n  [input -> \" + simplelist + \"output]\")\n",
        "\n",
        "    def strip(x):\n",
        "        return str(x).replace(\", \",',').replace(\"(\",'').replace(\")\",'') + \", \"\n",
        "    def n():\n",
        "        return \"  (\" + str(i) + \"): \" + \"nn.\" + str(l).split(\"(\", 1)[0]\n",
        "\n",
        "    for i, l in enumerate(net, 1):\n",
        "         if \"2d\" in str(l):\n",
        "             ks, st, pd = strip(l.kernel_size), strip(l.stride), strip(l.padding)\n",
        "             if \"Conv2d\" in str(l):\n",
        "                 ch = str(l.in_channels) + \" -> \" + str(l.out_channels)\n",
        "                 print(n() + \"(\" + ch + \", \" + (ks).replace(\",\",'x', 1) + st + pd.replace(\", \",')'))\n",
        "             elif \"Pool2d\" in str(l):\n",
        "                 st = st.replace(\"  \",' ') + st.replace(\", \",')')\n",
        "                 print(n() + \"(\" + ((ks).replace(\",\",'x' + ks, 1) + st).replace(\", \",','))\n",
        "         else:\n",
        "             print(n())\n",
        "    print(\")\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E1AH0rTusUf"
      },
      "source": [
        "## Print progress every `print_iter` iterations if `print_iter` not 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qe9sAsNupox"
      },
      "source": [
        "def print_iterations(t, loss):\n",
        "    if params.print_iter > 0 and t % params.print_iter == 0:\n",
        "        print(\"Iteration \" + str(t) + \" / \"+ str(params.num_iterations))\n",
        "        for i, loss_module in enumerate(content_losses):\n",
        "            print(\"  Content \" + str(i+1) + \" loss: \" + str(loss_module.loss.item()))\n",
        "        for i, loss_module in enumerate(style_losses):\n",
        "            print(\"  Style \" + str(i+1) + \" loss: \" + str(loss_module.loss.item()))\n",
        "        print(\"  Total loss: \" + str(loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUO_naomvH6L"
      },
      "source": [
        "## Save the image every `save_iter` iterations if `save_iter` not 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWM6uQgRvCo7"
      },
      "source": [
        "def save_image(t):\n",
        "    should_save = params.save_iter > 0 and t % params.save_iter == 0\n",
        "    should_save = should_save or t == params.num_iterations\n",
        "    if should_save:\n",
        "        output_filename, file_extension = os.path.splitext(params.output_image)\n",
        "        if t == params.num_iterations:\n",
        "            filename = output_filename + str(file_extension)\n",
        "        else:\n",
        "            filename = str(output_filename) + \"_\" + str(t) + str(file_extension)\n",
        "        disp = deprocess(img.clone())\n",
        "\n",
        "        # Maybe perform postprocessing for color-independent style transfer\n",
        "        if params.original_colors == 1:\n",
        "            disp = original_colors(deprocess(content_image.clone()), disp)\n",
        "\n",
        "        disp.save(str(filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oYF2_HMwiHO"
      },
      "source": [
        "## Function to evaluate loss and gradient \n",
        "We run the net forward and backward to get the gradient, and sum up losses from the loss modules.\n",
        "`optim.lbfgs` internally handles iteration and calls this function many times, so we manually count the number of iterations to handle printing and saving intermediate results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2POKiPtywhxv"
      },
      "source": [
        "num_calls = [0]\n",
        "def feval():\n",
        "    num_calls[0] += 1\n",
        "    optimizer.zero_grad()\n",
        "    net(img)\n",
        "    loss = 0\n",
        "\n",
        "    for mod in content_losses:\n",
        "        loss += mod.loss.to(backward_device)\n",
        "    for mod in style_losses:\n",
        "        loss += mod.loss.to(backward_device)\n",
        "    if params.tv_weight > 0:\n",
        "        for mod in tv_losses:\n",
        "            loss += mod.loss.to(backward_device)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    save_image(num_calls[0])\n",
        "    print_iterations(num_calls[0], loss)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gnl4EQfdn2I"
      },
      "source": [
        "## Divide weights by channel size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV28H2QpdofG"
      },
      "source": [
        "def normalize_weights(content_losses, style_losses):\n",
        "    for n, i in enumerate(content_losses):\n",
        "        i.strength = i.strength / max(i.target.size())\n",
        "    for n, i in enumerate(style_losses):\n",
        "        i.strength = i.strength / max(i.target.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4NtswQdr2N4"
      },
      "source": [
        "# **Experimentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU3cV6UHpnr7"
      },
      "source": [
        "dtype, multidevice, backward_device = setup_gpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7ycgS7Zpn3I",
        "outputId": "e3e8f2a5-e1a5-4da8-f09c-ef1f3187c6da"
      },
      "source": [
        "cnn, layerList = load_caffe_model(params.model_file, params.pooling, params.gpu, params.disable_check)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG-19 Architecture Detected\n",
            "Successfully loaded vgg19-d01eb7cb.pth\n",
            "conv1_1: 64 3 3 3\n",
            "conv1_2: 64 64 3 3\n",
            "conv2_1: 128 64 3 3\n",
            "conv2_2: 128 128 3 3\n",
            "conv3_1: 256 128 3 3\n",
            "conv3_2: 256 256 3 3\n",
            "conv3_3: 256 256 3 3\n",
            "conv3_4: 256 256 3 3\n",
            "conv4_1: 512 256 3 3\n",
            "conv4_2: 512 512 3 3\n",
            "conv4_3: 512 512 3 3\n",
            "conv4_4: 512 512 3 3\n",
            "conv5_1: 512 512 3 3\n",
            "conv5_2: 512 512 3 3\n",
            "conv5_3: 512 512 3 3\n",
            "conv5_4: 512 512 3 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZo6yf3rpn_p"
      },
      "source": [
        "content_image = preprocess(params.content_image, params.image_size).type(dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V7fbmmepoH2"
      },
      "source": [
        "style_image_input = params.style_image.split(',')\n",
        "style_image_list, ext = [], [\".jpg\", \".jpeg\", \".png\", \".tiff\"]\n",
        "for image in style_image_input:\n",
        "    if os.path.isdir(image):\n",
        "        images = (image + \"/\" + file for file in os.listdir(image)\n",
        "        if os.path.splitext(file)[1].lower() in ext)\n",
        "        style_image_list.extend(images)\n",
        "    else:\n",
        "        style_image_list.append(image)\n",
        "style_images_caffe = []\n",
        "for image in style_image_list:\n",
        "    style_size = int(params.image_size * params.style_scale)\n",
        "    img_caffe = preprocess(image, style_size).type(dtype)\n",
        "    style_images_caffe.append(img_caffe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGRzUlQKtJVF"
      },
      "source": [
        "if params.init_image != None:\n",
        "    image_size = (content_image.size(2), content_image.size(3))\n",
        "    init_image = preprocess(params.init_image, image_size).type(dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEdDb39UtYNH"
      },
      "source": [
        "Handle style blending weights for multiple style inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ris5NFGZtOUK"
      },
      "source": [
        "style_blend_weights = []\n",
        "if params.style_blend_weights == None:\n",
        "    # Style blending not specified, so use equal weighting\n",
        "    for i in style_image_list:\n",
        "        style_blend_weights.append(1.0)\n",
        "    for i, blend_weights in enumerate(style_blend_weights):\n",
        "        style_blend_weights[i] = int(style_blend_weights[i])\n",
        "else:\n",
        "    style_blend_weights = params.style_blend_weights.split(',')\n",
        "    assert len(style_blend_weights) == len(style_image_list), \\\n",
        "      \"-style_blend_weights and -style_images must have the same number of elements!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qNRrCdEtbTB"
      },
      "source": [
        "Normalize the style blending weights so they sum to 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlIfM3MCtV08"
      },
      "source": [
        "style_blend_sum = 0\n",
        "for i, blend_weights in enumerate(style_blend_weights):\n",
        "    style_blend_weights[i] = float(style_blend_weights[i])\n",
        "    style_blend_sum = float(style_blend_sum) + style_blend_weights[i]\n",
        "for i, blend_weights in enumerate(style_blend_weights):\n",
        "    style_blend_weights[i] = float(style_blend_weights[i]) / float(style_blend_sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGUdhvSCtiHH"
      },
      "source": [
        "content_layers = params.content_layers.split(',')\n",
        "style_layers = params.style_layers.split(',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggAXzRZ8tjaU"
      },
      "source": [
        "Set up the network, inserting style and content loss modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtvG5iudtlvO"
      },
      "source": [
        "cnn = copy.deepcopy(cnn)\n",
        "content_losses, style_losses, tv_losses = [], [], []\n",
        "next_content_idx, next_style_idx = 1, 1\n",
        "net = nn.Sequential()\n",
        "c, r = 0, 0\n",
        "\n",
        "if params.tv_weight > 0:\n",
        "    tv_mod = TVLoss(params.tv_weight).type(dtype)\n",
        "    net.add_module(str(len(net)), tv_mod)\n",
        "    tv_losses.append(tv_mod)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fToV2_xmtyqo",
        "outputId": "924f5b76-0050-4241-f934-519b364eba4e"
      },
      "source": [
        "for i, layer in enumerate(list(cnn), 1):\n",
        "    if next_content_idx <= len(content_layers) or next_style_idx <= len(style_layers):\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            net.add_module(str(len(net)), layer)\n",
        "\n",
        "            if layerList['C'][c] in content_layers:\n",
        "                print(\"Setting up content layer \" + str(i) + \": \" + str(layerList['C'][c]))\n",
        "                loss_module = ContentLoss(params.content_weight, params.normalize_gradients)\n",
        "                net.add_module(str(len(net)), loss_module)\n",
        "                content_losses.append(loss_module)\n",
        "\n",
        "            if layerList['C'][c] in style_layers:\n",
        "                print(\"Setting up style layer \" + str(i) + \": \" + str(layerList['C'][c]))\n",
        "                loss_module = StyleLoss(params.style_weight, params.normalize_gradients)\n",
        "                net.add_module(str(len(net)), loss_module)\n",
        "                style_losses.append(loss_module)\n",
        "            c+=1\n",
        "\n",
        "        if isinstance(layer, nn.ReLU):\n",
        "            net.add_module(str(len(net)), layer)\n",
        "\n",
        "            if layerList['R'][r] in content_layers:\n",
        "                print(\"Setting up content layer \" + str(i) + \": \" + str(layerList['R'][r]))\n",
        "                loss_module = ContentLoss(params.content_weight, params.normalize_gradients)\n",
        "                net.add_module(str(len(net)), loss_module)\n",
        "                content_losses.append(loss_module)\n",
        "                next_content_idx += 1\n",
        "\n",
        "            if layerList['R'][r] in style_layers:\n",
        "                print(\"Setting up style layer \" + str(i) + \": \" + str(layerList['R'][r]))\n",
        "                loss_module = StyleLoss(params.style_weight, params.normalize_gradients)\n",
        "                net.add_module(str(len(net)), loss_module)\n",
        "                style_losses.append(loss_module)\n",
        "                next_style_idx += 1\n",
        "            r+=1\n",
        "\n",
        "        if isinstance(layer, nn.MaxPool2d) or isinstance(layer, nn.AvgPool2d):\n",
        "            net.add_module(str(len(net)), layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting up style layer 2: relu1_1\n",
            "Setting up style layer 7: relu2_1\n",
            "Setting up style layer 12: relu3_1\n",
            "Setting up style layer 21: relu4_1\n",
            "Setting up content layer 23: relu4_2\n",
            "Setting up style layer 30: relu5_1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdLOuAfft2ow"
      },
      "source": [
        "if multidevice:\n",
        "    net = setup_multi_device(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VERT1MptuBQr"
      },
      "source": [
        "Capture content targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "509OguSgt4ue",
        "outputId": "4bf1b0a6-538f-4501-c69f-7657cf368516"
      },
      "source": [
        "for i in content_losses:\n",
        "    i.mode = 'capture'\n",
        "print(\"Capturing content targets\")\n",
        "print_torch(net, multidevice)\n",
        "net(content_image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Capturing content targets\n",
            "nn.Sequential ( \n",
            "  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> (12) -> (13) -> (14) -> (15) -> (16) -> (17) -> (18) -> (19) -> (20) -> (21) -> (22) -> (23) -> (24) -> (25) -> (26) -> (27) -> (28) -> (29) -> (30) -> (31) -> (32) -> (33) -> (34) -> (35) -> (36) -> (37) -> output]\n",
            "  (1): nn.TVLoss\n",
            "  (2): nn.Conv2d(3 -> 64, 3x3, 1,1, 1,1)\n",
            "  (3): nn.ReLU\n",
            "  (4): nn.StyleLoss\n",
            "  (5): nn.Conv2d(64 -> 64, 3x3, 1,1, 1,1)\n",
            "  (6): nn.ReLU\n",
            "  (7): nn.MaxPool2d(2x2, 2,2)\n",
            "  (8): nn.Conv2d(64 -> 128, 3x3, 1,1, 1,1)\n",
            "  (9): nn.ReLU\n",
            "  (10): nn.StyleLoss\n",
            "  (11): nn.Conv2d(128 -> 128, 3x3, 1,1, 1,1)\n",
            "  (12): nn.ReLU\n",
            "  (13): nn.MaxPool2d(2x2, 2,2)\n",
            "  (14): nn.Conv2d(128 -> 256, 3x3, 1,1, 1,1)\n",
            "  (15): nn.ReLU\n",
            "  (16): nn.StyleLoss\n",
            "  (17): nn.Conv2d(256 -> 256, 3x3, 1,1, 1,1)\n",
            "  (18): nn.ReLU\n",
            "  (19): nn.Conv2d(256 -> 256, 3x3, 1,1, 1,1)\n",
            "  (20): nn.ReLU\n",
            "  (21): nn.Conv2d(256 -> 256, 3x3, 1,1, 1,1)\n",
            "  (22): nn.ReLU\n",
            "  (23): nn.MaxPool2d(2x2, 2,2)\n",
            "  (24): nn.Conv2d(256 -> 512, 3x3, 1,1, 1,1)\n",
            "  (25): nn.ReLU\n",
            "  (26): nn.StyleLoss\n",
            "  (27): nn.Conv2d(512 -> 512, 3x3, 1,1, 1,1)\n",
            "  (28): nn.ReLU\n",
            "  (29): nn.ContentLoss\n",
            "  (30): nn.Conv2d(512 -> 512, 3x3, 1,1, 1,1)\n",
            "  (31): nn.ReLU\n",
            "  (32): nn.Conv2d(512 -> 512, 3x3, 1,1, 1,1)\n",
            "  (33): nn.ReLU\n",
            "  (34): nn.MaxPool2d(2x2, 2,2)\n",
            "  (35): nn.Conv2d(512 -> 512, 3x3, 1,1, 1,1)\n",
            "  (36): nn.ReLU\n",
            "  (37): nn.StyleLoss\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          ...,\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]],\n",
              "\n",
              "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 2.6878e+02,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 9.5606e+01,  ..., 1.2843e+02,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          ...,\n",
              "          [0.0000e+00, 0.0000e+00, 9.6218e+01,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]],\n",
              "\n",
              "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           3.2664e+02, 1.2402e+02],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           1.4981e+02, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          ...,\n",
              "          [0.0000e+00, 0.0000e+00, 4.3827e+01,  ..., 0.0000e+00,\n",
              "           2.5151e+01, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.9815e+01,\n",
              "           0.0000e+00, 0.0000e+00]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[0.0000e+00, 0.0000e+00, 1.9867e+02,  ..., 0.0000e+00,\n",
              "           6.5357e+01, 1.8447e+00],\n",
              "          [0.0000e+00, 3.0863e+01, 5.4747e+02,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [1.8739e+02, 1.5448e+02, 6.5857e+02,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          ...,\n",
              "          [0.0000e+00, 2.6970e+02, 6.3010e+02,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 4.2772e+02, 7.9671e+02,  ..., 3.5781e-01,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 1.9940e+02, 8.2862e+02,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]],\n",
              "\n",
              "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          ...,\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]],\n",
              "\n",
              "         [[0.0000e+00, 0.0000e+00, 2.9203e+01,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 5.6854e+02,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 3.3271e+02,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 1.9042e+02],\n",
              "          ...,\n",
              "          [0.0000e+00, 0.0000e+00, 3.8512e+02,  ..., 4.5768e+01,\n",
              "           3.9701e+01, 1.6789e+02],\n",
              "          [0.0000e+00, 0.0000e+00, 3.4287e+02,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 5.0983e+02,  ..., 1.5029e+02,\n",
              "           0.0000e+00, 0.0000e+00]]]], device='cuda:0',\n",
              "       grad_fn=<ReluBackward1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdIolXAduCgF"
      },
      "source": [
        "Capture style targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwsJSCT5t6ZK",
        "outputId": "191693ba-6568-46af-e092-d21c89cf8c60"
      },
      "source": [
        "for i in content_losses:\n",
        "    i.mode = 'None'\n",
        "\n",
        "for i, image in enumerate(style_images_caffe):\n",
        "    print(\"Capturing style target \" + str(i+1))\n",
        "    for j in style_losses:\n",
        "        j.mode = 'capture'\n",
        "        j.blend_weight = style_blend_weights[i]\n",
        "    net(style_images_caffe[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Capturing style target 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMKd5z3OuLHl"
      },
      "source": [
        "Set all loss modules to loss mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o67gCOk_t-SI"
      },
      "source": [
        "for i in content_losses:\n",
        "    i.mode = 'loss'\n",
        "for i in style_losses:\n",
        "    i.mode = 'loss'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eNXDpmruMzs"
      },
      "source": [
        "Normalize content and style weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWVKfzLwuNBT"
      },
      "source": [
        "if params.normalize_weights:\n",
        "    normalize_weights(content_losses, style_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0unIm9fXuNMX"
      },
      "source": [
        "Freeze the network in order to prevent unnecessary gradient calculations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoQgssMVuNZO"
      },
      "source": [
        "for param in net.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_HfI_SouNqO"
      },
      "source": [
        "Initialize the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB_1ff6QuN0g"
      },
      "source": [
        "if params.seed >= 0:\n",
        "    torch.manual_seed(params.seed)\n",
        "    torch.cuda.manual_seed_all(params.seed)\n",
        "    torch.backends.cudnn.deterministic=True\n",
        "if params.init == 'random':\n",
        "    B, C, H, W = content_image.size()\n",
        "    img = torch.randn(C, H, W).mul(0.001).unsqueeze(0).type(dtype)\n",
        "elif params.init == 'image':\n",
        "    if params.init_image != None:\n",
        "        img = init_image.clone()\n",
        "    else:\n",
        "        img = content_image.clone()\n",
        "img = nn.Parameter(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgN3eGiCgl2N",
        "outputId": "a030d1a6-c512-4489-c7d9-e041cac1f4d0"
      },
      "source": [
        "optimizer, loopVal = setup_optimizer(img)\n",
        "while num_calls[0] <= loopVal:\n",
        "      optimizer.step(feval)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running optimization with L-BFGS\n",
            "Iteration 100 / 1000\n",
            "  Content 1 loss: 1512603.5\n",
            "  Style 1 loss: 17190.15234375\n",
            "  Style 2 loss: 81972.3359375\n",
            "  Style 3 loss: 18371.2890625\n",
            "  Style 4 loss: 257689.890625\n",
            "  Style 5 loss: 516.2354736328125\n",
            "  Total loss: 1895934.625\n",
            "Iteration 200 / 1000\n",
            "  Content 1 loss: 1390582.5\n",
            "  Style 1 loss: 7014.34033203125\n",
            "  Style 2 loss: 23515.283203125\n",
            "  Style 3 loss: 8332.5927734375\n",
            "  Style 4 loss: 223809.15625\n",
            "  Style 5 loss: 587.9150390625\n",
            "  Total loss: 1661314.125\n",
            "Iteration 300 / 1000\n",
            "  Content 1 loss: 1356689.375\n",
            "  Style 1 loss: 2466.635986328125\n",
            "  Style 2 loss: 9472.1416015625\n",
            "  Style 3 loss: 6263.751953125\n",
            "  Style 4 loss: 219881.34375\n",
            "  Style 5 loss: 620.3319091796875\n",
            "  Total loss: 1602279.0\n",
            "Iteration 400 / 1000\n",
            "  Content 1 loss: 1345573.25\n",
            "  Style 1 loss: 1046.33837890625\n",
            "  Style 2 loss: 5879.7841796875\n",
            "  Style 3 loss: 5727.53466796875\n",
            "  Style 4 loss: 218761.53125\n",
            "  Style 5 loss: 632.0095825195312\n",
            "  Total loss: 1583992.0\n",
            "Iteration 500 / 1000\n",
            "  Content 1 loss: 1340640.75\n",
            "  Style 1 loss: 651.5587158203125\n",
            "  Style 2 loss: 4787.06591796875\n",
            "  Style 3 loss: 5565.48095703125\n",
            "  Style 4 loss: 218386.234375\n",
            "  Style 5 loss: 637.2774047851562\n",
            "  Total loss: 1576700.875\n",
            "Iteration 600 / 1000\n",
            "  Content 1 loss: 1337958.75\n",
            "  Style 1 loss: 516.415771484375\n",
            "  Style 2 loss: 4412.57421875\n",
            "  Style 3 loss: 5502.58642578125\n",
            "  Style 4 loss: 218291.59375\n",
            "  Style 5 loss: 640.5930786132812\n",
            "  Total loss: 1573162.0\n",
            "Iteration 700 / 1000\n",
            "  Content 1 loss: 1336203.75\n",
            "  Style 1 loss: 457.2980041503906\n",
            "  Style 2 loss: 4218.7109375\n",
            "  Style 3 loss: 5451.42578125\n",
            "  Style 4 loss: 218289.1875\n",
            "  Style 5 loss: 644.1856079101562\n",
            "  Total loss: 1570994.0\n",
            "Iteration 800 / 1000\n",
            "  Content 1 loss: 1335129.5\n",
            "  Style 1 loss: 428.3095397949219\n",
            "  Style 2 loss: 4123.748046875\n",
            "  Style 3 loss: 5410.81640625\n",
            "  Style 4 loss: 218192.328125\n",
            "  Style 5 loss: 645.5471801757812\n",
            "  Total loss: 1569593.5\n",
            "Iteration 900 / 1000\n",
            "  Content 1 loss: 1334249.75\n",
            "  Style 1 loss: 413.41790771484375\n",
            "  Style 2 loss: 4067.76318359375\n",
            "  Style 3 loss: 5393.92919921875\n",
            "  Style 4 loss: 218242.578125\n",
            "  Style 5 loss: 646.34375\n",
            "  Total loss: 1568634.0\n",
            "Iteration 1000 / 1000\n",
            "  Content 1 loss: 1333758.625\n",
            "  Style 1 loss: 406.17474365234375\n",
            "  Style 2 loss: 4027.560546875\n",
            "  Style 3 loss: 5383.080078125\n",
            "  Style 4 loss: 218131.609375\n",
            "  Style 5 loss: 647.3171997070312\n",
            "  Total loss: 1567942.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}